# -*- coding: utf-8 -*-
"""1st_asg_rag1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1msv9gREQbp1nh1vruePlV1duAR6dWxEF
"""

# Install required libraries
!pip install langchain pypdf faiss-cpu sentence-transformers transformers langchain_community

#Import libraries
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from transformers import pipeline
from langchain_community.llms import HuggingFacePipeline # Import HuggingFacePipeline

#Load the PDF file
loader = PyPDFLoader("/content/sample2.pdf")
documents=loader.load()

# Split text into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=200
)
texts = text_splitter.split_documents(documents)

# Create embeddings
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
db = FAISS.from_documents(texts,embeddings)

#  Load a local QA model
qa_model = pipeline(
    "text2text-generation",
    model="google/flan-t5-base",
    max_length=512
)

# Define a custom prompt
qa_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="Context: {context}\n\nQuestion: {question}\n\nAnswer:"
)

# Build the RetrievalQA chain
retriever = db.as_retriever(search_kwargs={"k": 2})

# Wrap the transformers pipeline with HuggingFacePipeline
llm_chain_model = HuggingFacePipeline(pipeline=qa_model)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm_chain_model, # Use the wrapped model
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt":qa_prompt}
)

# Ask questions in interactive mode
print("Ask questions about the PDF (type 'exit' to quit)\n")
while True:
    query = input("Your question: ")
    if query.lower() in ["exit", "quit", "q"]:
        print("Exiting...")
        break
    answer = qa_chain.invoke(query)
    print("Answer:",answer)